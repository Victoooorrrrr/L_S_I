{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from langdetect.detector_factory import DetectorFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = DetectorFactory()\n",
    "factory.load_profile('ld_profile')\n",
    " \n",
    "def detect(text):\n",
    "    detector = factory.create()\n",
    "    detector.append(text)\n",
    "    return detector.detect()\n",
    " \n",
    "def detect_langs(text):\n",
    "    detector = factory.create()\n",
    "    detector.append(text)\n",
    "    return detector.get_probabilities()\n",
    "\n",
    "factory.get_lang_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"count_courses_coursera.txt\",'r') # в данном файле должна быть записана последняя страница, которую мы знаем\n",
    "oldpg = f.read()\n",
    "oldpg = eval(oldpg)\n",
    "f.close()\n",
    "#Случай, когда страниц столько же, но возможно на странице прибавилось курсов(появились новые)\n",
    "#Всего на странице может находиться 35 курсов\n",
    "\n",
    "f = open(\"coursera_courses.txt\",'r')\n",
    "last_course = f.readlines()[-1]\n",
    "last_course = eval(last_course)[0]\n",
    "f.close()\n",
    "url1 = \"https://www.coursera.org/directory/courses?page=\" +str(oldpg)\n",
    "page = requests.get(url1)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "st1 = soup.find_all('a', class_ =\"MuiTypography-root MuiLink-root MuiLink-underlineHover css-19sqvu6 MuiTypography-colorPrimary\")\n",
    "f = open(\"coursera_courses.txt\",'a')\n",
    "for j in range(0, len(st1)):\n",
    "    #course-title\n",
    "    ct1 = soup.find_all('a', class_ =\"MuiTypography-root MuiLink-root MuiLink-underlineHover css-19sqvu6 MuiTypography-colorPrimary\")[j].get_text()\n",
    "    print(ct1[0:ct1.find('(')-1],len(ct1[0:ct1.find('(')-1]))\n",
    "    print(last_course,len(last_course))\n",
    "    if ct1[0:ct1.find('(')-1] == last_course:\n",
    "        print('DONE')\n",
    "        break;\n",
    "print('START')\n",
    "for k in range(j+1,len(st1)):\n",
    "    ct1 = soup.find_all('a', class_ =\"MuiTypography-root MuiLink-root MuiLink-underlineHover css-19sqvu6 MuiTypography-colorPrimary\")[k].get_text()\n",
    "    print(ct1)\n",
    "    if detect(ct1[0:ct1.find('(')-1]) == 'ru':\n",
    "        try:\n",
    "            #course URL\n",
    "            curl1 = soup.find_all(\"a\", class_='MuiTypography-root MuiLink-root MuiLink-underlineHover css-19sqvu6 MuiTypography-colorPrimary')[k]\n",
    "            f.write(f\"('''{ct1.split('(')[0].strip()}''', 'https://www.coursera.org{curl1.get('href')}', '''{ct1.split('(')[1][:-1].strip()}''')\\n\")\n",
    "                     #f.write(\"https://www.coursera.org\"+ curl.get('href') + '\\n')\n",
    "        except:\n",
    "             continue\n",
    "f.close()\n",
    "#берем юрл на каталог, заходим проверяем какая страница последняя \n",
    "#допустим последний раз у нас было 221 страниц, сейчас заходим и теперь их 222 \n",
    "url = \"https://www.coursera.org/directory/courses\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "newpg = soup.find_all('a',class_ =\"MuiTypography-root MuiLink-root MuiLink-underlineHover box number css-bu128x MuiTypography-colorPrimary\")[-1].get_text()\n",
    "\n",
    "f1 = open(\"coursera_courses.txt\",'a')\n",
    "\n",
    "#Случай, когда появилась новая страница. Нужно парсить эту страницу отдельно и добавлять ее в файл с остальными\n",
    "if newpg > oldpg:\n",
    "    for i in range(oldpg+1,newpg+1):\n",
    "        url1 = \"https://www.coursera.org/directory/courses?page=\" +str(i)\n",
    "        page = requests.get(url1)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        st1 = soup.find_all('a', class_ =\"MuiTypography-root MuiLink-root MuiLink-underlineHover css-19sqvu6 MuiTypography-colorPrimary\")\n",
    "        for j in range(0, len(st1)):\n",
    "            #course-title\n",
    "            ct1 = soup.find_all('a', class_ =\"MuiTypography-root MuiLink-root MuiLink-underlineHover css-19sqvu6 MuiTypography-colorPrimary\")[j].get_text()\n",
    "            #русский язык чек\n",
    "            if detect(ct1[0:ct1.find('(')-1]) == 'ru':\n",
    "                try:\n",
    "                    #course URL\n",
    "                    curl1 = soup.find_all(\"a\", class_='MuiTypography-root MuiLink-root MuiLink-underlineHover css-19sqvu6 MuiTypography-colorPrimary')[j]\n",
    "                    f1.write(f\"('''{ct1.split('(')[0].strip()}''', 'https://www.coursera.org{curl1.get('href')}', '''{ct1.split('(')[1][:-1].strip()}''')\\n\")\n",
    "                        #f.write(\"https://www.coursera.org\"+ curl.get('href') + '\\n')\n",
    "                except:\n",
    "                    continue\n",
    "f1 = open(\"count_courses_coursera.txt\",'w')\n",
    "f1.write(str(newpg))\n",
    "f1.close()\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
